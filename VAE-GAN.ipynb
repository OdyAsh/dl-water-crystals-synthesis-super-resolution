{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.data import Dataset\n",
    "from torch.autograd import Variable\n",
    "import os,cv2\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from torchvision.utils import make_grid \n",
    "from numpy import cov\n",
    "from numpy import trace\n",
    "from numpy import iscomplexobj\n",
    "from numpy import asarray\n",
    "from numpy.random import randint\n",
    "from scipy.linalg import sqrtm\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.datasets.mnist import load_data\n",
    "from skimage.transform import resize\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class waterCrtystals(Dataset):\n",
    "    def __init__(self, root, transform):\n",
    "        self.root = root\n",
    "        self.transform=transform\n",
    "    def __len__(self):\n",
    "        return len([entry for entry in os.listdir(self.root) if os.path.isfile(os.path.join(self.root, entry))])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        for i, img in enumerate(os.listdir(self.root)):\n",
    "            img = cv2.imread(os.path.join(f'{self.root}/{img}'))\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            if i == idx:\n",
    "                return img,idx\n",
    "\n",
    "\n",
    "def dataloader(batch_size):\n",
    "  dataroot=\"dataset/Microparticle\"\n",
    "  transform=transforms.Compose([transforms.ToTensor(), transforms.Resize(128),transforms.CenterCrop(128),transforms.Normalize((0),(1))])\n",
    "  dataset=waterCrtystals(root=dataroot,transform=transform)\n",
    "  data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "  return data_loader\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Encoder,self).__init__()\n",
    "    self.conv1=nn.Conv2d(3,64,5,padding=2,stride=2)   #in_channels=3\n",
    "    self.bn1=nn.BatchNorm2d(64,momentum=0.9)\n",
    "    self.conv2=nn.Conv2d(64,128,5,padding=2,stride=2)\n",
    "    self.bn2=nn.BatchNorm2d(128,momentum=0.9)\n",
    "    self.conv3=nn.Conv2d(128,256,5,padding=2,stride=2)\n",
    "    self.bn3=nn.BatchNorm2d(256,momentum=0.9)\n",
    "    self.relu=nn.LeakyReLU(0.2)\n",
    "    self.fc1=nn.Linear(256*16*16,2048)\n",
    "    self.bn4=nn.BatchNorm1d(2048,momentum=0.9)\n",
    "    self.fc_mean=nn.Linear(2048,128)\n",
    "    self.fc_logvar=nn.Linear(2048,128)   #latent dim=128\n",
    "  \n",
    "  def forward(self,x):\n",
    "    batch_size=x.size()[0]\n",
    "    out=self.relu(self.bn1(self.conv1(x)))\n",
    "    out=self.relu(self.bn2(self.conv2(out)))\n",
    "    out=self.relu(self.bn3(self.conv3(out)))\n",
    "    out=out.view(batch_size,-1)\n",
    "    out=self.relu(self.bn4(self.fc1(out)))\n",
    "    mean=self.fc_mean(out)\n",
    "    logvar=self.fc_logvar(out)\n",
    "    \n",
    "    return mean,logvar\n",
    "class Decoder(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Decoder,self).__init__()\n",
    "    self.fc1=nn.Linear(128,16*16*256)\n",
    "    self.bn1=nn.BatchNorm1d(16*16*256,momentum=0.9)\n",
    "    self.relu=nn.LeakyReLU(0.2)\n",
    "    self.deconv1=nn.ConvTranspose2d(256,256,6, stride=2, padding=2)\n",
    "    self.bn2=nn.BatchNorm2d(256,momentum=0.9)\n",
    "    self.deconv2=nn.ConvTranspose2d(256,128,6, stride=2, padding=2)\n",
    "    self.bn3=nn.BatchNorm2d(128,momentum=0.9)\n",
    "    self.deconv3=nn.ConvTranspose2d(128,32,6, stride=2, padding=2)\n",
    "    self.bn4=nn.BatchNorm2d(32,momentum=0.9)\n",
    "    self.deconv4=nn.ConvTranspose2d(32,3,5, stride=1, padding=2)\n",
    "    self.tanh=nn.Tanh()\n",
    "\n",
    "  def forward(self,x):\n",
    "    batch_size=x.size()[0]\n",
    "    x=self.relu(self.bn1(self.fc1(x)))\n",
    "    x=x.view(-1,256,16,16)\n",
    "    x=self.relu(self.bn2(self.deconv1(x)))\n",
    "    x=self.relu(self.bn3(self.deconv2(x)))\n",
    "    x=self.relu(self.bn4(self.deconv3(x)))\n",
    "    x=self.tanh(self.deconv4(x))\n",
    "    return x\n",
    "  \n",
    "class Discriminator(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Discriminator,self).__init__()\n",
    "    self.conv1=nn.Conv2d(3,32,5,padding=2,stride=1)\n",
    "    self.relu=nn.LeakyReLU(0.2)\n",
    "    self.conv2=nn.Conv2d(32,128,5,padding=2,stride=2)\n",
    "    self.bn1=nn.BatchNorm2d(128,momentum=0.9)\n",
    "    self.conv3=nn.Conv2d(128,256,5,padding=2,stride=2)\n",
    "    self.bn2=nn.BatchNorm2d(256,momentum=0.9)\n",
    "    self.conv4=nn.Conv2d(256,256,5,padding=2,stride=2)\n",
    "    self.bn3=nn.BatchNorm2d(256,momentum=0.9)\n",
    "    self.fc1=nn.Linear(16*16*256,512)\n",
    "    self.bn4=nn.BatchNorm1d(512,momentum=0.9)\n",
    "    self.fc2=nn.Linear(512,1)\n",
    "    self.sigmoid=nn.Sigmoid()\n",
    "\n",
    "  def forward(self,x):\n",
    "    batch_size=x.size()[0]\n",
    "    x=self.relu(self.conv1(x))\n",
    "    x=self.relu(self.bn1(self.conv2(x)))\n",
    "    x=self.relu(self.bn2(self.conv3(x)))\n",
    "    x=self.relu(self.bn3(self.conv4(x)))\n",
    "    x=x.view(-1,256*16*16)\n",
    "    x1=x;\n",
    "    x=self.relu(self.bn4(self.fc1(x)))\n",
    "    x=self.sigmoid(self.fc2(x))\n",
    "\n",
    "    return x,x1\n",
    "class VAE_GAN(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(VAE_GAN,self).__init__()\n",
    "    self.encoder=Encoder()\n",
    "    self.decoder=Decoder()\n",
    "    self.discriminator=Discriminator()\n",
    "    self.encoder.apply(weights_init)\n",
    "    self.decoder.apply(weights_init)\n",
    "    self.discriminator.apply(weights_init)\n",
    "\n",
    "\n",
    "  def forward(self,x):\n",
    "    bs=x.size()[0]\n",
    "    z_mean,z_logvar=self.encoder(x)\n",
    "    std = z_logvar.mul(0.5).exp_()\n",
    "        \n",
    "    #sampling epsilon from normal distribution\n",
    "    epsilon=Variable(torch.randn(bs,128)).to(device)\n",
    "    z=z_mean+std*epsilon\n",
    "    x_tilda=self.decoder(z)\n",
    "      \n",
    "    return z_mean,z_logvar,x_tilda\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_and_save(file_name,img):\n",
    "    npimg = np.transpose(img.numpy(),(1,2,0))\n",
    "    f = \"generated-VAEGANs/%s.png\" % file_name\n",
    "    fig = plt.figure(dpi=200)\n",
    "    fig.suptitle(file_name, fontsize=14, fontweight='bold')\n",
    "    #plt.imshow(npimg)\n",
    "    plt.imsave(f,npimg)\n",
    "def plot_loss(loss_list):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.title(\"Loss During Training\")\n",
    "    plt.plot(loss_list,label=\"Loss\")\n",
    "    \n",
    "    plt.xlabel(\"iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale an array of images to a new size\n",
    "def scale_images(images, new_shape):\n",
    " images_list = list()\n",
    " for image in images:\n",
    "       # resize with nearest neighbor interpolation\n",
    "       new_image = resize(image, new_shape, 0)\n",
    "       # store\n",
    "       images_list.append(new_image)\n",
    " return asarray(images_list)\n",
    " \n",
    "# calculate frechet inception distance\n",
    "def calculate_fid(model, images1, images2):\n",
    " # calculate activations\n",
    " act1 = model.predict(images1)\n",
    " act2 = model.predict(images2)\n",
    " # calculate mean and covariance statistics\n",
    " mu1, sigma1 = act1.mean(axis=0), cov(act1, rowvar=False)\n",
    " mu2, sigma2 = act2.mean(axis=0), cov(act2, rowvar=False)\n",
    " # calculate sum squared difference between means\n",
    " ssdiff = np.sum((mu1 - mu2)**2.0)\n",
    " # calculate sqrt of product between cov\n",
    " covmean = sqrtm(sigma1.dot(sigma2))\n",
    " # check and correct imaginary numbers from sqrt\n",
    " if iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    " # calculate score\n",
    " fid = ssdiff + trace(sigma1 + sigma2 - 2.0 * covmean)\n",
    " return fid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared (10, 32, 32, 3) (10, 32, 32, 3)\n",
      "Scaled (10, 128, 128, 3) (10, 128, 128, 3)\n",
      "1/1 [==============================] - 1s 989ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#TEST\n",
    "\n",
    "# prepare the inception v3 model\n",
    "model = InceptionV3(include_top=False, pooling='avg', input_shape=(128,128,3))\n",
    "# define two fake collections of images\n",
    "images1 = randint(0, 255, 10*32*32*3)\n",
    "images1 = images1.reshape((10,32,32,3))\n",
    "images2 = randint(0, 255, 10*32*32*3)\n",
    "images2 = images2.reshape((10,32,32,3))\n",
    "print('Prepared', images1.shape, images2.shape)\n",
    "# convert integer to floating point values\n",
    "images1 = images1.astype('float32')\n",
    "images2 = images2.astype('float32')\n",
    "# resize images\n",
    "images1 = scale_images(images1, (128,128,3))\n",
    "images2 = scale_images(images2, (128,128,3))\n",
    "print('Scaled', images1.shape, images2.shape)\n",
    "# pre-process images\n",
    "images1 = preprocess_input(images1)\n",
    "images2 = preprocess_input(images2)\n",
    "# fid between images1 and images1\n",
    "fid = calculate_fid(model, images1, images1)\n",
    "print('FID (same): %.3f' % fid)\n",
    "# fid between images1 and images2\n",
    "fid = calculate_fid(model, images1, images2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shehab\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 944ms/step\n",
      "1/1 [==============================] - 0s 155ms/step\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model = InceptionV3(include_top=False, pooling='avg', input_shape=(128,128,3))\n",
    "data_loader=dataloader(20)\n",
    "gen=VAE_GAN().to(device)\n",
    "discrim=Discriminator().to(device)\n",
    "real_batch = next(iter(data_loader))\n",
    "show_and_save(\"training\" ,make_grid((real_batch[0]*0.5+0.5).cpu(),8))\n",
    "epochs=100\n",
    "lr=3e-4\n",
    "alpha=0.1\n",
    "gamma=15\n",
    "\n",
    "criterion=nn.BCELoss().to(device)\n",
    "optim_E=torch.optim.RMSprop(gen.encoder.parameters(), lr=lr)\n",
    "optim_D=torch.optim.RMSprop(gen.decoder.parameters(), lr=lr)\n",
    "optim_Dis=torch.optim.RMSprop(discrim.parameters(), lr=lr*alpha)\n",
    "z_fixed=Variable(torch.randn((64,128))).to(device)\n",
    "x_fixed=Variable(real_batch[0]).to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  prior_loss_list,gan_loss_list,recon_loss_list=[],[],[]\n",
    "  dis_real_list,dis_fake_list,dis_prior_list=[],[],[]\n",
    "  fid_scores=[]\n",
    "  for i, (data,_) in enumerate(data_loader, 0):\n",
    "    bs=data.size()[0]\n",
    "    \n",
    "    ones_label=Variable(torch.ones(bs,1)).to(device)\n",
    "    zeros_label=Variable(torch.zeros(bs,1)).to(device)\n",
    "    zeros_label1=Variable(torch.zeros(10,1)).to(device)\n",
    "    datav = Variable(data).to(device)\n",
    "    mean, logvar, rec_enc = gen(datav)\n",
    "    z_p = Variable(torch.randn(10,128)).to(device)\n",
    "    x_p_tilda = gen.decoder(z_p)\n",
    " \n",
    "    output = discrim(datav)[0]\n",
    "    errD_real = criterion(output, ones_label)\n",
    "    dis_real_list.append(errD_real.item())\n",
    "    output = discrim(rec_enc)[0]\n",
    "    errD_rec_enc = criterion(output, zeros_label)\n",
    "    dis_fake_list.append(errD_rec_enc.item())\n",
    "    output = discrim(x_p_tilda)[0]\n",
    "    errD_rec_noise = criterion(output, zeros_label1)\n",
    "    dis_prior_list.append(errD_rec_noise.item())\n",
    "    gan_loss = errD_real + errD_rec_enc + errD_rec_noise\n",
    "    gan_loss_list.append(gan_loss.item())\n",
    "    optim_Dis.zero_grad()\n",
    "    gan_loss.backward(retain_graph=True)\n",
    "    optim_Dis.step()\n",
    "\n",
    "\n",
    "    output = discrim(datav)[0]\n",
    "    errD_real = criterion(output, ones_label)\n",
    "    output = discrim(rec_enc)[0]\n",
    "    errD_rec_enc = criterion(output, zeros_label)\n",
    "    output = discrim(x_p_tilda)[0]\n",
    "    errD_rec_noise = criterion(output, zeros_label1)\n",
    "    gan_loss = errD_real + errD_rec_enc + errD_rec_noise\n",
    "    \n",
    "\n",
    "    x_l_tilda = discrim(rec_enc)[1]\n",
    "    x_l = discrim(datav)[1]\n",
    "    rec_loss = ((x_l_tilda - x_l) ** 2).mean()\n",
    "    err_dec = gamma * rec_loss - gan_loss \n",
    "    recon_loss_list.append(rec_loss.item())\n",
    "    optim_D.zero_grad()\n",
    "    err_dec.backward(retain_graph=True)\n",
    "    optim_D.step()\n",
    "    \n",
    "    mean, logvar, rec_enc = gen(datav)\n",
    "    x_l_tilda = discrim(rec_enc)[1]\n",
    "    x_l = discrim(datav)[1]\n",
    "    rec_loss = ((x_l_tilda - x_l) ** 2).mean()\n",
    "    prior_loss = 1 + logvar - mean.pow(2) - logvar.exp()\n",
    "    prior_loss = (-0.5 * torch.sum(prior_loss))/torch.numel(mean.data)\n",
    "    prior_loss_list.append(prior_loss.item())\n",
    "    err_enc = prior_loss + 5*rec_loss\n",
    "\n",
    "    b=gen(x_fixed)[2]\n",
    "    b=b.detach()\n",
    "    c=gen.decoder(z_fixed)\n",
    "    c=c.detach()\n",
    "\n",
    "    real = scale_images(data, (128,128,3))\n",
    "    fake = scale_images(b, (128,128,3))\n",
    "    fid_score = calculate_fid(model, real, fake)\n",
    "    fid_scores.append(fid_score)\n",
    "    \n",
    "    optim_E.zero_grad()\n",
    "    err_enc.backward(retain_graph=True)\n",
    "    optim_E.step()\n",
    "\n",
    "    if i % 50 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_gan: %.4f\\tLoss_prior: %.4f\\tRec_loss: %.4f\\tdis_real_loss: %0.4f\\tdis_fake_loss: %.4f\\tdis_prior_loss: %.4f\\tFID: %.4f'\n",
    "                  % (epoch,epochs, i, len(data_loader),\n",
    "                     gan_loss.item(), prior_loss.item(),rec_loss.item(),errD_real.item(),errD_rec_enc.item(),errD_rec_noise.item(), fid_score))\n",
    "\n",
    "\n",
    "  show_and_save('noise_epoch_%d.png' % epoch ,make_grid((c*0.5+0.5).cpu(),8))\n",
    "  show_and_save('epoch_%d.png' % epoch ,make_grid((b*0.5+0.5).cpu(),8))\n",
    "  \n",
    "plot_loss(prior_loss_list)\n",
    "plot_loss(recon_loss_list)\n",
    "plot_loss(gan_loss_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
